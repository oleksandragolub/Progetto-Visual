{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14655512,"sourceType":"datasetVersion","datasetId":9362349},{"sourceId":14688854,"sourceType":"datasetVersion","datasetId":9383621},{"sourceId":14876985,"sourceType":"datasetVersion","datasetId":9517656},{"sourceId":755151,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":576658,"modelId":588964}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Imports required for TTA from the notebook\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# ==========================================\n# 1. CONFIGURATION\n# ==========================================\nDEGRADED_TEST_DIR = \"/kaggle/input/datasets/andreaspagnolo/visual-exam-dataset-2/visual_dataset/test_degradato\"\nMODEL_PATH = \"/kaggle/input/models/andreaspagnolo/task2-denoisetestset/pytorch/default/1/best_model_task2_denoiseTestSet.pth\"\nBATCH_SIZE = 64\nNUM_CLASSES = 100\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Normalization constants used in the notebook\nMEAN = [0.485, 0.456, 0.406]\nSTD  = [0.229, 0.224, 0.225]\n\nprint(f\"Using device: {DEVICE}\")\n\n# ==========================================\n# 2. MODEL DEFINITION\n# ==========================================\ndef build_resnet18(num_classes: int) -> nn.Module:\n    model = models.resnet18(weights=None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\ndef load_model(path, num_classes, device):\n    model = build_resnet18(num_classes)\n    try:\n        checkpoint = torch.load(path, map_location=device)\n        if 'model_state_dict' in checkpoint:\n            state_dict = checkpoint['model_state_dict']\n        else:\n            state_dict = checkpoint\n        model.load_state_dict(state_dict)\n        print(\"Model weights loaded successfully.\")\n    except Exception as e:\n        print(f\"[ERROR] Unable to load weights: {e}\")\n        return None\n    model.to(device)\n    model.eval()\n    return model\n\n# ==========================================\n# 3. TRANSFORMATIONS (Standard vs TTA)\n# ==========================================\n\n# --- A. Standard PyTorch Transforms (No TTA) ---\ntest_transforms_standard = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(MEAN, STD),\n])\n\n# --- B. TTA Transforms (Albumentations from Notebook) ---\ndef build_tta_transforms() -> list:\n    \"\"\"\n    Returns the list of 10 augmentations defined in the input notebook.\n    \"\"\"\n    norm = [A.Normalize(mean=MEAN, std=STD), ToTensorV2()]\n    return [\n        A.Compose([A.Resize(256, 256), A.CenterCrop(224, 224), *norm]),\n        A.Compose([A.Resize(256, 256), A.CenterCrop(224, 224),\n                   A.HorizontalFlip(p=1), *norm]),\n        A.Compose([A.Resize(256, 256), A.CenterCrop(224, 224),\n                   A.Sharpen(alpha=(0.3, 0.5), lightness=(0.8, 1.2), p=1), *norm]),\n        A.Compose([A.Resize(256, 256), A.CenterCrop(224, 224),\n                   A.HorizontalFlip(p=1),\n                   A.Sharpen(alpha=(0.5, 0.7), lightness=(0.9, 1.1), p=1), *norm]),\n        A.Compose([A.Resize(256, 256), A.CenterCrop(224, 224),\n                   A.CLAHE(clip_limit=3.0, tile_grid_size=(8, 8), p=1), *norm]),\n        A.Compose([A.Resize(256, 256), A.CenterCrop(224, 224),\n                   A.HorizontalFlip(p=1),\n                   A.CLAHE(clip_limit=3.0, tile_grid_size=(8, 8), p=1), *norm]),\n        A.Compose([A.Resize(288, 288), A.CenterCrop(224, 224), *norm]),\n        A.Compose([A.Resize(288, 288), A.CenterCrop(224, 224),\n                   A.HorizontalFlip(p=1), *norm]),\n        A.Compose([A.Resize(256, 256), A.RandomCrop(224, 224), *norm]),\n        A.Compose([A.Resize(256, 256), A.CenterCrop(224, 224),\n                   A.UnsharpMask(blur_limit=(3, 5), sigma_limit=(0.0, 1.0),\n                                 alpha=(0.2, 0.5), p=1), *norm]),\n    ]\n\n@torch.no_grad()\ndef predict_tta(model, img_np: np.ndarray, tta_tfs: list, device) -> int:\n    \"\"\"\n    Applies TTA transforms, aggregates probabilities, and returns best class index.\n    \"\"\"\n    model.eval()\n    probs_list = []\n    \n    # We use basic pytorch autocast if available for speed, though not strictly required for inference\n    # Note: If autocast gives issues on CPU, it safely ignores usually, but here we check device\n    use_amp = (device.type == 'cuda')\n    \n    for tf in tta_tfs:\n        # Apply albumentations transform\n        augmented = tf(image=img_np.copy())[\"image\"]\n        t = augmented.unsqueeze(0).to(device)\n        \n        with torch.amp.autocast('cuda', enabled=use_amp):\n            # Model output\n            output = model(t)\n            # Softmax to get probabilities\n            p = F.softmax(output, dim=1)\n            \n        probs_list.append(p.cpu().float().numpy())\n    \n    # Average predictions across all TTA versions\n    mean_probs = np.mean(probs_list, axis=0)\n    return int(np.argmax(mean_probs))\n\n# ==========================================\n# 4. METRICS & PLOTTING UTILS\n# ==========================================\ndef calculate_and_print_metrics(targets, preds, class_names, title=\"Metrics\"):\n    print(f\"\\n--- {title} ---\")\n    \n    report_dict = classification_report(\n        targets, preds, target_names=class_names, output_dict=True, zero_division=0\n    )\n    \n    accuracy = report_dict['accuracy']\n    macro = report_dict['macro avg']\n    weighted = report_dict['weighted avg']\n    \n    print(f\"Overall Accuracy:   {accuracy:.4f} ({accuracy*100:.2f}%)\")\n    print(\"-\" * 40)\n    print(\"MACRO AVERAGE:\")\n    print(f\"  Precision: {macro['precision']:.4f}\")\n    print(f\"  Recall:    {macro['recall']:.4f}\")\n    print(f\"  F1-Score:  {macro['f1-score']:.4f}\")\n    print(\"-\" * 40)\n    print(\"WEIGHTED AVERAGE:\")\n    print(f\"  Precision: {weighted['precision']:.4f}\")\n    print(f\"  Recall:    {weighted['recall']:.4f}\")\n    print(f\"  F1-Score:  {weighted['f1-score']:.4f}\")\n    print(\"=\" * 40)\n    \n    return report_dict\n\ndef plot_confusion_matrix(targets, preds, class_names, output_filename):\n    cm = confusion_matrix(targets, preds)\n    plt.figure(figsize=(20, 20))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names, cbar=False)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {output_filename}')\n    plt.tight_layout()\n    plt.savefig(output_filename, dpi=100)\n    print(f\"Confusion matrix saved to {output_filename}\")\n    plt.close()\n\n# ==========================================\n# 5. EVALUATION PIPELINES\n# ==========================================\n\ndef run_evaluation_standard(model, dataset, device):\n    \"\"\"Standard evaluation using DataLoader (No TTA)\"\"\"\n    print(\"\\n[Mode: Standard (No TTA)] Starting inference...\")\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n    \n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(loader, desc=\"Standard Eval\"):\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(labels.numpy())\n            \n    return all_targets, all_preds\n\ndef run_evaluation_tta(model, dataset, device):\n    \"\"\"Evaluation using Test Time Augmentation (TTA)\"\"\"\n    print(\"\\n[Mode: TTA (Test Time Augmentation)] Starting inference...\")\n    \n    # Initialize TTA transforms\n    tta_transforms = build_tta_transforms()\n    \n    all_preds = []\n    all_targets = []\n    \n    # Iterate through samples directly to apply custom TTA logic\n    # dataset.samples contains list of (image_path, label_index)\n    samples = dataset.samples\n    \n    for img_path, label in tqdm(samples, desc=\"TTA Eval\"):\n        # Load image as Numpy (RGB) to match Albumentations requirement\n        # Note: PIL.Image -> np.array is what the notebook used\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        \n        # Get prediction\n        pred_idx = predict_tta(model, image, tta_transforms, device)\n        \n        all_preds.append(pred_idx)\n        all_targets.append(label)\n        \n    return all_targets, all_preds\n\n# ==========================================\n# 6. MAIN EXECUTION\n# ==========================================\ndef main():\n    if not os.path.exists(DEGRADED_TEST_DIR):\n        print(f\"[ERROR] Directory not found: {DEGRADED_TEST_DIR}\")\n        return\n\n    # 1. Load Model\n    # We use the standard dataset class just to get class names and file paths easily\n    # For standard eval, we pass the standard transform. \n    # For TTA, we ignore the transform in the dataset and load raw images.\n    base_dataset = datasets.ImageFolder(DEGRADED_TEST_DIR, transform=test_transforms_standard)\n    class_names = base_dataset.classes\n    print(f\"Classes: {len(class_names)}\")\n    print(f\"Total Images: {len(base_dataset)}\")\n\n    model = load_model(MODEL_PATH, NUM_CLASSES, DEVICE)\n    if model is None: return\n\n    # ---------------------------------------------------------\n    # 2. Run Standard Evaluation (No TTA)\n    # ---------------------------------------------------------\n    targets_std, preds_std = run_evaluation_standard(model, base_dataset, DEVICE)\n    metrics_std = calculate_and_print_metrics(targets_std, preds_std, class_names, title=\"RESULTS: NO TTA\")\n    plot_confusion_matrix(targets_std, preds_std, class_names, \"cm_no_tta.png\")\n\n    # ---------------------------------------------------------\n    # 3. Run TTA Evaluation\n    # ---------------------------------------------------------\n    targets_tta, preds_tta = run_evaluation_tta(model, base_dataset, DEVICE)\n    metrics_tta = calculate_and_print_metrics(targets_tta, preds_tta, class_names, title=\"RESULTS: WITH TTA\")\n    plot_confusion_matrix(targets_tta, preds_tta, class_names, \"cm_with_tta.png\")\n\n    # ---------------------------------------------------------\n    # 4. Final Comparison\n    # ---------------------------------------------------------\n    acc_no_tta = metrics_std['accuracy'] * 100\n    acc_with_tta = metrics_tta['accuracy'] * 100\n    diff = acc_with_tta - acc_no_tta\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"             FINAL COMPARISON\")\n    print(\"=\"*50)\n    print(f\"Accuracy (No TTA):   {acc_no_tta:.2f}%\")\n    print(f\"Accuracy (With TTA): {acc_with_tta:.2f}%\")\n    print(f\"Improvement:         {diff:+.2f}%\")\n    print(\"=\"*50)\n    \n    # Save comparison to CSV\n    df_std = pd.DataFrame(metrics_std).transpose()\n    df_std.to_csv(\"metrics_no_tta.csv\")\n    \n    df_tta = pd.DataFrame(metrics_tta).transpose()\n    df_tta.to_csv(\"metrics_with_tta.csv\")\n    print(\"\\nMetrics saved to 'metrics_no_tta.csv' and 'metrics_with_tta.csv'\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-18T15:52:42.997194Z","iopub.execute_input":"2026-02-18T15:52:42.997545Z","iopub.status.idle":"2026-02-18T15:53:40.211454Z","shell.execute_reply.started":"2026-02-18T15:52:42.997515Z","shell.execute_reply":"2026-02-18T15:53:40.210725Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nClasses: 100\nTotal Images: 500\nModel weights loaded successfully.\n\n[Mode: Standard (No TTA)] Starting inference...\n","output_type":"stream"},{"name":"stderr","text":"Standard Eval: 100%|██████████| 8/8 [00:01<00:00,  7.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- RESULTS: NO TTA ---\nOverall Accuracy:   0.7880 (78.80%)\n----------------------------------------\nMACRO AVERAGE:\n  Precision: 0.8181\n  Recall:    0.7880\n  F1-Score:  0.7796\n----------------------------------------\nWEIGHTED AVERAGE:\n  Precision: 0.8181\n  Recall:    0.7880\n  F1-Score:  0.7796\n========================================\nConfusion matrix saved to cm_no_tta.png\n\n[Mode: TTA (Test Time Augmentation)] Starting inference...\n","output_type":"stream"},{"name":"stderr","text":"TTA Eval: 100%|██████████| 500/500 [00:26<00:00, 19.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- RESULTS: WITH TTA ---\nOverall Accuracy:   0.8160 (81.60%)\n----------------------------------------\nMACRO AVERAGE:\n  Precision: 0.8423\n  Recall:    0.8160\n  F1-Score:  0.8110\n----------------------------------------\nWEIGHTED AVERAGE:\n  Precision: 0.8423\n  Recall:    0.8160\n  F1-Score:  0.8110\n========================================\nConfusion matrix saved to cm_with_tta.png\n\n==================================================\n             FINAL COMPARISON\n==================================================\nAccuracy (No TTA):   78.80%\nAccuracy (With TTA): 81.60%\nImprovement:         +2.80%\n==================================================\n\nMetrics saved to 'metrics_no_tta.csv' and 'metrics_with_tta.csv'\n","output_type":"stream"}],"execution_count":4}]}